
Linear regression: tutorial 1
========================================================
author: Adnane Ez-zizi
date: 21 March 2018
autosize: true

Does this R output look familiar?
========================================================

```{r, echo = FALSE}
Advertising <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv")
fit1 = lm(sales ~ TV, data = Advertising)
summary(fit1)
```

Example
========================================================

Imagine we have data about the sales of a product in 200 different
markets, along with advertising budgets for three different media: TV, radio, and newspaper. 

```{r}
Advertising <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv")
head(Advertising, n=3)
```

Questions 
========================================================

1) Is their a relationship between advertising budget and sales?  

2) How strong is the relationship between advertising budget and sales? 

3) Which media contribute to sales?  

4) How accurately can we estimate the effect of each medium on sales?  

Questions 
========================================================

5) How accurately can we predict future sales?  

6) Is the relationship linear?  

7) Is there synergy among the advertising media?  

Task: mlu data set
========================================================

- Load the mlu data set. 
- How many variables and observations are there?

**Data set description:** 

- During language acquisition, it is claimed that caregivers adapt their language to the language abilities of children. The data set follows a particular child and record an hour conversation between the child and the mother once every month between the child's second and fourth birth dates. 

- The variables chi and mot record, for both the child and the mother, a measure of language complexity/competency for children called Mean Length of Utterance (MLU).


The general form of a statistical model
========================================================

Let $Y$ be the output variable (e.g. sales), and $X$ the input variables $X^1, X^2, ..., X^p$ (e.g., budget for each advertising medium). We assume that there is a relationship between $Y$ and $X$, which can be represented by:

\begin{equation}
Y = f(X) + \epsilon
\end{equation}

where $\epsilon$ is an unavoidable noise that is independent of $X$.

**Objective:** Given some observed data $(Y_1,X_1), (Y_2,X_2), ..., (Y_n,X_n)$, we want to estimate $f$ and evaluate our estimate. 

The general form of a model in statistics
========================================================

Once we have an estimate $\hat{f}$ for $f$, we can predict unavailable values of $Y$ for new values of $X$: 

\begin{equation}
\hat{Y} = \hat{f}(X_{new})
\end{equation}

Example:

```{r, echo=FALSE}
data.frame(TV = c(300), radio = c(20), newspaper = c(70), sales = c("?")) 
```

We can predict the sales as: $\hat{sales} = \hat{f}(300,20,70)$

Simple linear regression
========================================================

Simple linear regression refers to the special case where we approximate $f$ using a linear function and where there is only one independent variable $X$ (e.g., TV budget). In this case the previous model becomes:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}

Now our task is to estimate the parameters $\beta_0$ and $\beta_1$, which is much easier than estimating the whole function $f$. 

Simple linear regression
========================================================

Back to our example: Is their a relationship between TV advertising budget and sales? And if yes, is the relationship linear?

```{r, echo=FALSE}
  plot(sales~TV, Advertising)
```

Task: Plot based on the mlu data
========================================================
   
   
- Plot the relationship between the mother's and child's MLU.
- Does the relationship look linear?

Simple linear regression
========================================================

How to choose a regression line that best fits the data? 

```{r, echo=FALSE}
  plot(sales~TV, Advertising)
  abline(1.3, 0.093, col = "red")
```


Simple linear regression
========================================================

Maybe this one?

```{r, echo=FALSE}
  plot(sales~TV, Advertising)
  abline(4, 0.07, col = "red")
```

Simple linear regression: Method of least squares
========================================================
incremental: false

The least square error for each line fit is computed as follows:

1. Choose an arbitrary line. This will produce a predicted value $\hat{Y}_i$ for each of your outcome points $Y_i$. 

2. Compute the error (or residual) that this generates for each of your points: $e_i = Y_i - \hat{Y}_i$ 

3. Sum the squares of these errors $e_1$, $e_2$, ..., $e_n$: $RSS = e_1^2 + e_2^2 + ... + e_n^2$

Finally, we pick the line that produces the minimum sum of square errors. 

Simple linear regression: fit in R
========================================================

```{r}
fit1 = lm(sales ~ TV, data = Advertising)
summary(fit1)
```


Simple linear regression: Assessing the accuracy of the coefficient estimates
========================================================

Remember the simple linear model takes the form:

\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}

**Coefficient estimates:** $\hat{\beta}_0$ estimates the intercept term---the expected value of $Y$ when $X = 0$, and $\hat{\beta}_1$ estimates the slope---the average increase in $Y$ associated with a one-unit increase in $X$.

Simple linear regression: Assessing the accuracy of the coefficient estimates
========================================================
   
   
**Coefficient standard errors:** measure how far off each coefficient estimate is from the true coefficent. Mathematically speaking, if you have many samples from which you compute each time the coefficient estimate. This will produce a distribution of estimates. The standard deviation of the resulting distribution is called standard error of the coefficient estimate. 

Simple linear regression: Assessing the accuracy of the coefficient estimates
========================================================

**Hypothesis testing:** 
Does X (e.g. TV ads) affects Y (sales)? To answer this question, we test the null hypothesis    
$H_0$: "There is no relationship between X and Y"   
versus the alternative hypothesis   
$H_1$: "There is some relationship between X and Y".

Mathematically, this corresponds to testing:   
$H_0$: "$\beta_1 = 0$"   
versus the alternative hypothesis    
$H_1$: "$\beta_1 \neq 0$".

Hypothesis testing compared to a proof by contradiction
======================================================== 
  
**Proof by contradiction**

- *Premise:* If $H_0$, then $d$ does not occur;  
- *Data:* $d$ occurs;  
- *Conclusion:* $H_0$ is not true.  

Example: Alibi (being elsewhere at the time of the crime) 

***
  
**Hypothesis testing**

- *Premise:* If $H_0$, then $d$ is very unlikely;  
- *Data:* $d$ occurs;  
- *Conclusion:* $H_0$ is very unlikely.  

Example: Alibi (being elsewhere near the time of the crime)

p-value
========================================================

The *p-value* measures how much inconsistent the data are with the null hypothesis (the probability of seeing a less consistent outcome than what you observed if $H_0$ were true). The lower is the *p-value*, the strongest the evidence against the null hypothesis.

```{r, echo = FALSE}
fit1 = lm(sales ~ TV, data = Advertising)
summary(fit1)
```

Simple linear regression: Assessing the model fit
========================================================

**Residual standard error:** It is the average amount by which the dependent variable deviates from the true regression line. 

\begin{equation}
RSE = \sqrt{RSS/(n-2)}
\end{equation}

where $RSS = e_1^2 + e_2^2 + ... + e_n^2 = \sum_{i=1}^{n}e_i^2$. It is clear that the smaller is the $RSE$, the better.

Simple linear regression: Assessing the model fit
========================================================

**R-squared:** It is the fraction of variance explained by the model

\begin{equation}
R\^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
\end{equation}

where 
- $TSS = \sum_{i=1}^{n}(y_i-\bar{y})^2$ measures the total amount of variability inherent in the response before the regression is performed. 
- $RSS$ measures the amount of variability that is left unexplained after performing the regression. 
- $TSS - RSS$ measures the amount of variability in the response that is explained (or removed) by performing the regression. 
- $R^2$ therefore measures the proportion of variability in $Y$ that can be explained using $X$.

Simple linear regression: Another way to see R-squared
========================================================

*In simple linear regression*, R-squared is simply the squared correlation between the dependent variable $Y$ and the independent variable $X$:

\begin{equation}
R\^2 = cor(X,Y)
\end{equation}

How does the R output look like now?
========================================================

```{r, echo = FALSE}
fit1 = lm(sales ~ TV, data = Advertising)
summary(fit1)
```

Task: Simple linear model for the mlu data
========================================================
   
   
- Build a simple linear regression model to try to explain mot as a function of chi.
- What can you conclude?


<!-- Now given the values of the $RSE$ (=3.26) and $R^2$ (0.61) in our example, how good is the model fit? -->

<!-- ```{r lmSim, fig.show='animate', echo=FALSE} -->

<!-- library(animation) -->
<!-- oopt = ani.options(interval = 0.3, nmax = 10) -->
<!-- out <- vector("list", 10) -->

<!-- for (i in 1:ani.options("nmax")) { -->
<!--   plot(sales~TV, Advertising) -->
<!--   abline(runif(1, 5, 10), runif(1, 0, 0.2), col = "red") -->
<!--   ani.pause() -->
<!-- } -->
<!-- ani.options(oopt) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE} -->
<!-- for (i in 1:10) { -->
<!--   plot(sales~TV, Advertising) -->
<!--   abline(runif(1, 5, 10), runif(1, 0, 0.2), col = "red") -->
<!--   date_time<-Sys.time() -->
<!--   while((as.numeric(Sys.time()) - as.numeric(date_time))<0.5){} -->
<!-- } -->
<!-- ``` -->
